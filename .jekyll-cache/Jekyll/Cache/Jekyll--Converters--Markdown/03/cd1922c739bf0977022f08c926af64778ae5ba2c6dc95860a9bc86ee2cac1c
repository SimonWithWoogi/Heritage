I"E/<p><a href="https://simonwithwoogi.github.io/SimonWithWoogi.github.io/posts/matlabneuralnet2/">전편:Forward propagation</a></p>

<p><span style="color:red">이번 포스팅에는 귀여운 그림이 전혀 없습니다.</span></p>

<h2 id="1-forward-propagation"><span style="color:darkblue">1. Forward propagation</span></h2>

<p><img src="/assets/img/MATLAB/5_11.png" alt="img" /></p>

<p>[\begin{array}{l}
h_1\ =\ w^1<em>{11}x_1+w^1</em>{21}x_2+c_1<br />
h_2\ =\ w^1<em>{12}x_1+w^1</em>{22}x_2+c_2<br />
<br />
\hat{h} = \hat{w}^{1’}\hat{x}+\hat{c}<br />
\hat{h} = \begin{bmatrix}
h_{1}<br />
h_{2}
\end{bmatrix}\qquad
\hat{w}^1 = \begin{bmatrix}
w^1<em>{11}\ w^1</em>{12}<br />
w^1<em>{21}\ w^1</em>{22}<br />
\end{bmatrix} \qquad
\hat{x} = \begin{bmatrix}
x_{1}<br />
x_{2}
\end{bmatrix}<br />
<br />
\therefore \ y=\hat{w}^{2’}\hat{h}+b<br />
\hat{w}^2=  \begin{bmatrix}
w^2<em>{11} <br />
w^2</em>{21}
\end{bmatrix}</p>

<p>\end{array}\]</p>

<p>구조는 이렇습니다. 사실 <code class="language-plaintext highlighter-rouge">MLP</code> 는 간단하지 않기때문에 처음 <code class="language-plaintext highlighter-rouge">순전파(forward propagation)</code> 부터 설명하겠습니다. 순서대로 계산한다는 개념이죠. 이번에는 <code class="language-plaintext highlighter-rouge">input node</code> 가 2개니까 <code class="language-plaintext highlighter-rouge">weight</code> 를 모두 0.5씩 걸어두고 진행해보겠습니다. <code class="language-plaintext highlighter-rouge">c와 b</code> 는 <code class="language-plaintext highlighter-rouge">bias</code> 개념이니까 1부터 시작하겠습니다. 계산은 <code class="language-plaintext highlighter-rouge">MATLAB</code>으로 합니다.</p>

<p><img src="/assets/img/MATLAB/5_12.png" alt="img" /></p>

<table>
  <thead>
    <tr>
      <th>X1</th>
      <th>X2</th>
      <th>h1</th>
      <th>h2</th>
      <th>Y(Predict)</th>
      <th>Y(Real)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <td>0</td>
      <td>1</td>
      <td>1.5</td>
      <td>1.5</td>
      <td>2.5</td>
      <td>1</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0</td>
      <td>1.5</td>
      <td>1.5</td>
      <td>2.5</td>
      <td>1</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>3</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<p>제법 많이 다르죠? <code class="language-plaintext highlighter-rouge">Sign function</code> 을 적용하기에도, 즉  <code class="language-plaintext highlighter-rouge">0과 1로</code> 부호화하기가 어렵습니다. <code class="language-plaintext highlighter-rouge">Layer</code> 를 추가했으니 <code class="language-plaintext highlighter-rouge">h</code> 를 기준으로 다시 표를 그려보겠습니다.</p>

<table>
  <thead>
    <tr>
      <th>h1</th>
      <th>h2</th>
      <th>Y(Predict)</th>
      <th>Y(Real)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <td>1.5</td>
      <td>1.5</td>
      <td>2.5</td>
      <td>1</td>
    </tr>
    <tr>
      <td>2.0</td>
      <td>2.0</td>
      <td>3</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<p>이렇게보니 <code class="language-plaintext highlighter-rouge">h1, h2</code> 가 너무 똑같이 나오네요. <code class="language-plaintext highlighter-rouge">c</code> 를 조정할 필요가 있어보입니다. 그래서 보통 <code class="language-plaintext highlighter-rouge">MLP</code> 에 들어가는 초기 <code class="language-plaintext highlighter-rouge">c와 weight</code> 는 이런 이유로 균일하게 구성하지 않습니다. 사실 똑같이 구성해도 큰 문제가 없으나 결과가 조금 느리게 나올 뿐입니다.</p>

<p><code class="language-plaintext highlighter-rouge">c를 1과 0으로 두개 구성했다면 h1 = [1 1.5 2.0]이고 h2=[0. 0.5 1.0]이니 h1과 h2를 조합하여 잘 사용할 수 있습니다. 지금은 두 h가 똑같아 하나를 없애도 괜찮겠다라는 생각이 들게 합니다.</code></p>

<h2 id="2-backward-propagation"><span style="color:darkblue">2. Backward propagation</span></h2>

<p>[E(w) = \frac{1}{2}(f(x;w)-y)^2]</p>

<p><code class="language-plaintext highlighter-rouge">Back propagation</code> 은 <code class="language-plaintext highlighter-rouge">Weight</code> 를 조정하는 과정입니다. 반복하면서 실제와 예측의 차이를 줄여나가는 과정입니다. <code class="language-plaintext highlighter-rouge">Weight</code> 를 좋은 방향으로 조정한 다음 다시 계산하면, 아무래도 좋은 결과가 나오겠죠?</p>

<h3 id="21-define-cost-function"><span style="color:darkblue">2.1. Define cost function</span></h3>

<p>[\begin{array}{l}
E(w) = \frac{1}{2}(f(x;w)-y)^2 <br />
f(x;w) = \text{Forward propagation 결과}<br />
\text{기울기 w는 편미분을 통해 얻어진다.}<br />
\triangledown w = \begin{bmatrix}
\frac{\partial E}{\partial w_1}\ \frac{\partial E}{\partial w_2} \cdots \frac{\partial E}{\partial w_n} <br />
\end{bmatrix}<br />
\vartriangle w_i = -1\cdot\eta\frac{\partial E}{\partial w_i}<br />
0&lt;\eta&lt;1
\end{array}]</p>

<p><code class="language-plaintext highlighter-rouge">비용함수(Cost function)</code> 는 위와 같이 이해할 수 있습니다. <code class="language-plaintext highlighter-rouge">Forward propagation</code> 에서 얻어진 결과와 실제 <code class="language-plaintext highlighter-rouge">y</code> 와 차이라고 볼 수 있습니다. 그러면 기울기에 대해서 이해해볼까요 1, 1 -&gt; 2, 2 -&gt; 3, 3으로 진행되면 오른쪽 대각선 기울기가 만들어지겠네요.</p>

<p><img src="/assets/img/MATLAB/6_1.png" alt="img" /></p>

<p>그럼 3,3 -&gt; 2,2 -&gt; 1,1로 진행되면 또 오른쪽 대각선 기울기가 만들어지겠네요. 한쪽은 감소하고, 한쪽은 증가하는데, 기울기가 같은 모양이죠?</p>

<p>그리고 또 <code class="language-plaintext highlighter-rouge">Cost function</code> 을 봅시다. 결과적으로는 <code class="language-plaintext highlighter-rouge">Back propagtaion</code> 은 <code class="language-plaintext highlighter-rouge">Weight</code> 를 조정하는 과정이라고 했습니다. 기울기를 구한다고 해서 <code class="language-plaintext highlighter-rouge">Weight</code> 를 어떻게 조정할 수 있을까요? 기울기란 그저 막대기가 얼마나 눕혀있는 지만 얘기하는데 말이죠. 많이 기울어져 있다면 <code class="language-plaintext highlighter-rouge">Weight</code> 는 높은 값을 줘야할까요? 호기심이 생깁니다.</p>

<h3 id="22-gradient-descentdelta-rule"><span style="color:darkblue">2.2. Gradient descent(delta rule)</span></h3>

<p><img src="/assets/img/MATLAB/6_2.png" alt="img" /></p>

<p><code class="language-plaintext highlighter-rouge">1번째 시도</code>에 값이 <code class="language-plaintext highlighter-rouge">1</code>, <code class="language-plaintext highlighter-rouge">2번째 시도</code>에 값이 <code class="language-plaintext highlighter-rouge">2</code>, <code class="language-plaintext highlighter-rouge">3번째 시도</code>에 값이 <code class="language-plaintext highlighter-rouge">3</code> 이라고 합시다. <code class="language-plaintext highlighter-rouge">번째</code> 는 시도를 의미하니 <code class="language-plaintext highlighter-rouge">x</code> 축으로 보고 <code class="language-plaintext highlighter-rouge">y</code> 는 <code class="language-plaintext highlighter-rouge">Cost function</code> 의 결과라고 봅니다. 그럼 이걸로 <code class="language-plaintext highlighter-rouge">기울기(Gradient)</code> 를 찾아봅시다. 단, 기울기를 통해서 제일 작은 값으로 내려가야 합니다. 왜냐하면 <code class="language-plaintext highlighter-rouge">Cost function</code> 이 실제 결과와의 차이를 의미하기 때문입니다.</p>

<p>사실 상승 기울기가 나오면 내려가야 하는 상황입니다. 왜냐하면 최소값을 찾아야해서요. 하강 기울기가 나오면 올라가야 하는 상황입니다. 그래서 아래와 같이 결과값에 대해서 <code class="language-plaintext highlighter-rouge">-1</code> 을 곱하게 됩니다.
\(\begin{array}{l}
\vartriangle w_i = -1\cdot\eta\frac{\partial E}{\partial w_i}\\
0&lt;\eta&lt;1
\end{array}\)
<code class="language-plaintext highlighter-rouge">Delta rule</code> 을 같이 이해하셔도 됩니다. 단어만 다르게 쓸 뿐이지. 개념적인 부분은 똑같습니다. 근데 왜 <code class="language-plaintext highlighter-rouge">Delta</code> 일까요. <code class="language-plaintext highlighter-rouge">Gradient descent</code> 처럼 변화량을 보기 때문이죠. <code class="language-plaintext highlighter-rouge">Delta rule</code> 은 아래와 같은 수식으로 정의합니다. 다만 이건 수식일 뿐이지 <code class="language-plaintext highlighter-rouge">기울기</code> 에 집중하는 것도 똑같고 <code class="language-plaintext highlighter-rouge">Cost function</code> 을  <code class="language-plaintext highlighter-rouge">Learning Error</code> 로 얘기합니다. 좀 더 <code class="language-plaintext highlighter-rouge">인공지능스러운</code> 단어로 전환한 것이죠. 추가로 <code class="language-plaintext highlighter-rouge">Delta function</code> 에서는 실제 값을 알 수 없을 때, 즉 중간층에서 적용합니다.</p>

<p>[\begin{array}{l}
\text{Delta rule}<br />
D_N={(x^{(d)},\ y^{(d)})}^N_{d=1}\qquad\text{N개의 학습 데이터}<br />
f^{(d)}=f(x^{(d)};w)=\text{activation funciton}(\Sigma^n_{i=0}w_ix_i)\qquad\text{학습모델 form}<br />
E_d\ =\ \frac{1}{2}(f^{(d)}-y^{(d)})^2\qquad\text{Learning Error}<br />
E_N\ =\ \Sigma^N_{d=1}E_d
\end{array}]</p>

<h3 id="23-chain-rule"><span style="color:darkblue">2.3. Chain rule</span></h3>

<p>[(h(x))’=(g(f(x)))’=g’(f(x))\cdot f’(x)]</p>

<p>이번 설명에서는 <code class="language-plaintext highlighter-rouge">Activation function</code> 을 빠뜨렸죠? 이제 슬슬 나옵니다. <code class="language-plaintext highlighter-rouge">SLP</code> 와 <code class="language-plaintext highlighter-rouge">MLP</code> 모두 <code class="language-plaintext highlighter-rouge">Sum</code> 을 하고 난 뒤에는 <code class="language-plaintext highlighter-rouge">Activation function</code> 을 거쳐지나 갑니다. <code class="language-plaintext highlighter-rouge">Sum function</code> 을 <code class="language-plaintext highlighter-rouge">f(x)</code> 라고 한다면, <code class="language-plaintext highlighter-rouge">Activation function</code> 을 <code class="language-plaintext highlighter-rouge">g(x)</code> 라고 한다면, <code class="language-plaintext highlighter-rouge">한 노드에만 두 개의 함수</code>가 있습니다. <code class="language-plaintext highlighter-rouge">Delta rule</code> 만 봐도, 저희는 <code class="language-plaintext highlighter-rouge">미분</code> 을 해야하는데 함수가 두개가 있어 순서대로 미분해야하는 상황이 생깁니다. 그 순서대로 미분하는 것이 <code class="language-plaintext highlighter-rouge">Chain rule</code> 입니다.</p>

<p>[\frac{\partial z}{\partial w}\ =\ \frac{\partial z}{\partial y}\frac{\partial y}{\partial x}\frac{\partial x}{\partial w}]</p>

<p><img src="/assets/img/MATLAB/6_3.png" alt="img" /></p>

<h3 id="24-activation-functions"><span style="color:darkblue">2.4. Activation functions</span></h3>

<p>마지막으로 거쳐가는 Activation function에 <code class="language-plaintext highlighter-rouge">sign function</code> 을 제외한 여러 함수들이 있고 <code class="language-plaintext highlighter-rouge">도함수</code> 가 어떻게 적용되는 지 아래를 확인해주세요.</p>

<p><img src="/assets/img/MATLAB/6_4.png" alt="img" /></p>
:ET