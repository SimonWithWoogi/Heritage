I"<h2 id="0-introduction"><span style="color:darkblue">0. Introduction</span></h2>

<p><code class="language-plaintext highlighter-rouge">SVM(Support Vector Machine)</code> 은 맨 처음 <code class="language-plaintext highlighter-rouge">generalized portrait algorithm</code> 이라 불렸습니다. 그리고 1990년대 끝에서 <code class="language-plaintext highlighter-rouge">Kernel trick</code> 을 이용하여 <code class="language-plaintext highlighter-rouge">SVM</code> 에 적용한 논문, 더 나아가 <code class="language-plaintext highlighter-rouge">Soft margin</code> 을 이용한 <code class="language-plaintext highlighter-rouge">SVM</code> 이 나오면서 현재 <code class="language-plaintext highlighter-rouge">Machine learning</code> 에 사용하는 <code class="language-plaintext highlighter-rouge">SVM</code> 으로 굳어지게 됩니다. 그러니 이번 포스팅은 <code class="language-plaintext highlighter-rouge">Kernel method</code> 에 대한 설명으로 시작하여 <code class="language-plaintext highlighter-rouge">SVM</code> 을 이용한 <code class="language-plaintext highlighter-rouge">MATLAB</code> 실습으로 마무리합니다.</p>

<h2 id="1-kernel-trick"><span style="color:darkblue">1. Kernel Trick</span></h2>

<p><code class="language-plaintext highlighter-rouge">Kernel method</code> 는 <code class="language-plaintext highlighter-rouge">Memory</code> 기반 처리입니다. 학습이 끝나면 학습 데이터를 어느정도 저장하고 있다가 예측 단계에서 사용하죠. 사실 이 <code class="language-plaintext highlighter-rouge">Kernel method</code> 가 매우 강력하여 1990년대에 <code class="language-plaintext highlighter-rouge">Perceptron vs MachineLearning</code> 의 싸움에서 인공신경망인 <code class="language-plaintext highlighter-rouge">Perceptron</code> 을 깔끔하게 눌러버립니다. 결국 <code class="language-plaintext highlighter-rouge">Deep learning</code> 의 성능때문에 밀렸지만, 그 때 당시엔 인기였습니다. <strong>지금은 <code class="language-plaintext highlighter-rouge">Deep learning</code> 과 함께 어우러져 쓰고 있습니다.</strong></p>

<h3 id="11-trick-meaning"><span style="color:darkblue">1.1. Trick meaning</span></h3>

<p><img src="/assets/img/MATLAB/10_1.png" alt="함정카드" /></p>

<p><strong>아니, 이런 학술적인 용어에 트릭이요?</strong></p>

<p>먼저, <code class="language-plaintext highlighter-rouge">운영체제(OS:Operating System)</code> 에서의 하드웨어와 중개자역할인 <code class="language-plaintext highlighter-rouge">Kernel</code> 과 개념적으로 유사하나 그에 대한 설명이 아닙니다. <code class="language-plaintext highlighter-rouge">Kernel Trick</code> 은 <strong>실제 차원에 대해서 공간을 변환하지 않고 우회하여 변환하는 효과를 주는 것입니다.</strong>  말그대로 <code class="language-plaintext highlighter-rouge">Trick</code> 이라서 <code class="language-plaintext highlighter-rouge">선형 분리</code> 가 불가능한 공간을 <code class="language-plaintext highlighter-rouge">선형 분리</code> 가 가능한 차원 혹은 공간 변환의 효과를 내는 것이죠. 우회를 자세히 얘기하면, 수리적인 방법으로 접근하여 변환된 공간을 매핑하지않고 내적을 얻는다고 말할 수 있습니다.</p>

<h3 id="12-kernel-function"><span style="color:darkblue">1.2. Kernel function</span></h3>

<p><code class="language-plaintext highlighter-rouge">ILSVRC(ImageNet Large Scale Visual Recognition Competition )</code> 에서 우승한 신경망 입력 사이즈를 확인해보면 보통 <code class="language-plaintext highlighter-rouge">224x224</code> 를 사용합니다. 사이즈를 따로 수정하지 않고 한 픽셀들이 곧 특징이고 차원으로 받아들일 수 있는데, 그렇다면 <code class="language-plaintext highlighter-rouge">50176</code> 차원이 나오게 됩니다.</p>

<p><img src="/assets/img/MATLAB/10_2.png" alt="XOR2to3" /></p>

<p>XOR의 문제는 3차원으로 변환하면서 구분이 가능하듯, 고차원을 통하면 분류에 있어서 이점은 확실히 있습니다. 그런데 <code class="language-plaintext highlighter-rouge">50176</code> 차원은 충분히 고차원이고 여기서 더 고차원으로 바꾸는 일은 쉽지 않습니다. 여기서는 <code class="language-plaintext highlighter-rouge">Kernel function</code> 이 차원변환에 도움을 줍니다. 일단 <code class="language-plaintext highlighter-rouge">Kernel function</code> 의 형태는 아래와 같습니다. x, z 가 특징 벡터라면 두 독립변수의 내적한 값을 가져온다는 얘기이고 이 <code class="language-plaintext highlighter-rouge">Kernel function</code> 의 핵심 또한 <strong>내적</strong> 을 구한다에 있습니다.</p>

<p>[K(x, z) = \phi(x) \cdot \phi(z)]</p>

<hr />

<p><strong>유명한 세 가지 <code class="language-plaintext highlighter-rouge">Kernel function</code></strong></p>

<p>[\begin{array}{l}
\text{Polynomial Kernel : } K(x,z)\ =\ (x\cdot z +1)^p <br />
\text{RBF Kernel : } K(x,z)\ =\ \exp(\frac{-||x-z||_2^2}{2\sigma^2}) <br />
\text{Hyperbolic tangent Kernel : } = K(x,z)\ =\ \text{tanh}(\alpha x\cdot z + \beta)</p>

<p>\end{array}]</p>

<hr />

<h3 id="13-core-points"><span style="color:darkblue">1.3. Core points </span></h3>

<p><code class="language-plaintext highlighter-rouge">Kernel function</code> 을 사용하면 내적을 구하는 연산량이 줄어들게 됩니다. 변환할 때의 매핑을 하지 않기 때문인데요. 기저함수를 이용하는 것보다 훨씬 효율적인 면이 있습니다. 결론적으로 <code class="language-plaintext highlighter-rouge">Kernel trick</code> 이란 매핑없이 변환 공간의 내적을 얻는 방법이고 이는 기존 공간 그대로 있으면서 선형적으로 분리 가능한 고차원 공간의 특성을 사용함을 의미합니다. 그래도 <code class="language-plaintext highlighter-rouge">Kernel trick</code> 이 만능은 아닙니다. 사용하려면 중요한 전제가 있는데요. 변환된 공간의 연산이 내적으로 표현되어야 합니다.</p>

<p>솔직히 <code class="language-plaintext highlighter-rouge">Kernel function</code> 에 대해서 많은 내용이 생략됐습니다. 왜 <code class="language-plaintext highlighter-rouge">Kernel</code> 이라 부르고 <code class="language-plaintext highlighter-rouge">Kernel</code> 이 무엇인가, 기존 고차원으로 변환할때는 연산이 얼마나 많이 걸릴까, 기저함수는 어떻게 정의할 수 있을까 등등, 그러나 이번 포스팅의 주제에 많이 벗어나는 것 같아 잘랐습니다. 왜냐하면 <code class="language-plaintext highlighter-rouge">Kernel trick</code> 을 <code class="language-plaintext highlighter-rouge">아주 성공적으로 이용한 사례인 SVM</code> 에 대한 느낌으로 얘기하는 것이 아니라 <code class="language-plaintext highlighter-rouge">SVM의 메커니즘을 이해하고 실습하기</code> 에 초점을 맞추고 싶었습니다.</p>

<h2 id="2-support-vector-machinesvm"><span style="color:darkblue">2. Support Vector Machine(SVM) </span></h2>

<p><img src="/assets/img/MATLAB/10_3.png" alt="근육맛쿠키" /></p>

<p><code class="language-plaintext highlighter-rouge">Kernel trick</code> 이 붙은 <code class="language-plaintext highlighter-rouge">SVM</code> 은 더이상 무서울 것이 없습니다. 이건 뭐 그냥 깡패에요. <code class="language-plaintext highlighter-rouge">Kernel trick</code> 한번 적용됐다고 얘를 데리고 다니면서 처음엔 그저 <code class="language-plaintext highlighter-rouge">분류(classification)</code> 로만 쓰다가 <code class="language-plaintext highlighter-rouge">회귀(Regression)</code> 까지 지도 학습 모든 분야에 자리르 잡습니다.</p>

<h3 id="21-generalized-classfier"><span style="color:darkblue">2.1. Generalized classfier </span></h3>

<p><code class="language-plaintext highlighter-rouge">SVM</code> 은 처음 <code class="language-plaintext highlighter-rouge">generalized portrait algorithm</code> 이라 불렸던 만큼, 일반화 능력에 대해서 먼저 얘기를 하겠습니다.</p>

<p><img src="/assets/img/MATLAB/10_4.png" alt="분류일반화" /></p>

<p>위 산점도를 기준으로 1번은 실패한 분류기, 2번과 3번은 잘 분류된 분류기라 볼 수 있습니다. <strong>하지만 여기서 어떤 것이 새로운 데이터가 들어왔을 때 3번이 더 잘 분류할 것으로 보입니다.</strong> 왜냐하면 2번은 한쪽 집단에 너무 붙어있어서 불안하죠? 물론 데이터를 다 까보면 실제로 2번에 피팅됐을 수도 있습니다. 그러나 SVM의 핵심은 현재 가지고 있는 데이터에서 각 집단간 여백을 최대하 함에 있습니다.</p>

<p><img src="/assets/img/MATLAB/10_5.png" alt="그게아니죠" /></p>

<h3 id="22-linear-support-vector-machine"><span style="color:darkblue">2.2. Linear Support Vector Machine </span></h3>

<p>특정 선을 기준으로 집단을 나눌때, 그 선을 <code class="language-plaintext highlighter-rouge">결정경계(Decision boundary)</code> 라고 부르겠습니다. 먼저 집단이 2개라고 가정할때 <code class="language-plaintext highlighter-rouge">결정경계</code> 의 식은 아래와 같습니다.</p>

<p>[d(x) = w^Tx+b=0 <br />
w = \text{weight} <br />
d(x) = \text{points in dimension}]</p>

<p><code class="language-plaintext highlighter-rouge">경사하강법(Gradient Decent)</code> 에서, <code class="language-plaintext highlighter-rouge">Neural Network</code>, <code class="language-plaintext highlighter-rouge">Fully connected network</code> 에서 주로 보는 식입니다. <code class="language-plaintext highlighter-rouge">d(x) = 0</code> 으로 인하여, 양수면 A 집단, 음수면 B 집단으로 구분할 수 있습니다. 그리고 w는 기울기의 각도라면 b는 기울기의 위치를 결정합니다. 이제 b가 각 집단 여백의 딱 가운데에 있도록 조정합니다. <strong>그리고 그 여백의 딱 가운데에 있도록 도와주는 벡터를 Support Vector라고 합니다.</strong></p>

<p><img src="/assets/img/MATLAB/10_6.png" alt="SV" /></p>

<h3 id="23-margin"><span style="color:darkblue">2.3. Margin </span></h3>

<p>이제는 <code class="language-plaintext highlighter-rouge">Margin(여백)</code> 이 가장 큰 기울기를 어떻게 계산하는 지 말씀드리겠습니다. w로 기울기 방향이 잘 잡혀있어야 기본적으로 좋은 모델일테니까요. 먼저 거리 계산식은 <code class="language-plaintext highlighter-rouge">L2 Norm</code> 을 이용합니다.</p>

<p>[\begin{array}{l}
L_2 &amp; = \sqrt {\sum_i^n x_i^2} \ 
&amp; =  \sqrt {x_1^2 + x_2^2 + x_3^2 + …. + x_n^2}
\end{array}]</p>

<p>x 를 <code class="language-plaintext highlighter-rouge">Support Vector</code> 라고 했을때, 여백은 아래의 수식으로 표현할 수 있습니다. 그리고 <code class="language-plaintext highlighter-rouge">d(x)가 음수면 A집단 / 양수면 B집단</code> 이라 했을 때 <code class="language-plaintext highlighter-rouge">d(x)</code> 를 1로 두었을 때 계산하기 쉬워 최종적으로 나오는 식에 참고해주세요.</p>

<p>[\begin{array}{l}
\text{Margin} &amp;= 2\cdot\frac{|d(x)|}{||w||_2}<br />
&amp;= \frac{2}{||w||_2}
\end{array}]</p>

<p>이제 이 <code class="language-plaintext highlighter-rouge">Margin</code> 을 최대화할때, 여기저기 <code class="language-plaintext highlighter-rouge">Support Vector</code> 들을 바꿔가면서 어디가 제일 큰 여백일까 찾아보게 됩니다. 이를 <code class="language-plaintext highlighter-rouge">조건부 최적화(Conditional optimization)</code> 이라고 부르며 <code class="language-plaintext highlighter-rouge">라그랑주 승수(Lagurange multiplier)</code> 를 이용하여 해결합니다. 마지막으로 <code class="language-plaintext highlighter-rouge">Wolfe dual</code> 을 이용하여 내적이 나타나도록 수식을 바꿉니다. 수식 설명은 그리 어렵지 않지만 내용만 차지하기에 생략하겠습니다. 의미하는 바는 <code class="language-plaintext highlighter-rouge">w, b</code> 를 제거하고 <code class="language-plaintext highlighter-rouge">라그랑주 승수 a</code> 만 남아 간단한 연산이 됩니다.</p>

<h3 id="24-soft-margin"><span style="color:darkblue">2.4. Soft Margin </span></h3>

<p><img src="/assets/img/MATLAB/10_7.png" alt="선형분리불가능" /></p>

<p>살다보면 선형분리가 불가능한 상황이 더 많습니다. <code class="language-plaintext highlighter-rouge">XOR</code> 문제만봐도 그렇지요. <code class="language-plaintext highlighter-rouge">선형 SVM</code> 은 선형분리가 가능한 2가지 분류로 가정했습니다. 그러다보니까 <code class="language-plaintext highlighter-rouge">Margin</code> 안에는 어떤 점도 찍히지 않았습니다.  <strong><code class="language-plaintext highlighter-rouge">Soft margin</code> 은 선형분리가 안 되는 걸 받아들이고 <code class="language-plaintext highlighter-rouge">Margin</code> 안에 데이터를 허용하는 아이디어 입니다.</strong></p>

<hr />

<p>존재 가능하며 고려할 필요있는 모든 데이터</p>

<ul>
  <li>
    <p>잘 분류됐으며 <code class="language-plaintext highlighter-rouge">결정경계 + 여백</code> 에 들어와있는 경우</p>
  </li>
  <li>오분류됐으며 <code class="language-plaintext highlighter-rouge">결정경계 + 여백</code> 에 들어와있는 경우</li>
  <li>잘 분류됐으며 <code class="language-plaintext highlighter-rouge">여백</code> 밖에 있는 경우</li>
  <li>오분류됐으며 <code class="language-plaintext highlighter-rouge">여백</code> 밖에 있는 경우</li>
</ul>

<hr />

<p>위 네가지 경우를 하나의 수식으로 정리할 수 있습니다. <code class="language-plaintext highlighter-rouge">슬랙변수(Slack variable)</code> 를 추가하여 정리 가능합니다.</p>

<p>[\begin{array}{l}
\text{Slack variable} = \xi <br />
1-\xi \leq y(w^tx+b)<br />
1.\  \xi=0, \text{잘 분류된 여백밖 데이터}<br />
2.\ 0&lt;\xi\leq=1,\text{잘 분류된 여백 안 데이터}<br />
3.\ 1&lt;\xi,\text{오분류된 모든 데이터}
\end{array}]</p>

<p>이제 여백을 크게하면서 <code class="language-plaintext highlighter-rouge">슬랙변수</code> 가 0에 피팅하는 w를 찾으면 되겠습니다.</p>

<p>[J(w,\xi)=\frac{1}{2}||w||^2<em>2+C\sum</em>{i=1}^{n}\xi_i <br />
\text{첫째항은 여백을 크게, 둘째항은 슬랙변수가 0이 되게}<br />
C=\text{hyper parameter}]</p>

<p><code class="language-plaintext highlighter-rouge">Hyper parameter</code> 에 따라 데이터 정확도 혹은 오분류율에 얼마나 민감하게 반응할 지 조정할 수 있습니다.</p>

<p>이후로는 <code class="language-plaintext highlighter-rouge">라그랑주 승수</code> 로 변환하고, <code class="language-plaintext highlighter-rouge">Wolfe 쌍대 문제</code> 로 다시 작성합니다. 결론은 <code class="language-plaintext highlighter-rouge">2.3. Margin</code> 의 내용에서 C의 역할이 생겼다는 것으로 이해하시면 됩니다.</p>

<h3 id="25-non-linear-svm"><span style="color:darkblue">2.5. Non linear SVM </span></h3>

<p><code class="language-plaintext highlighter-rouge">비선형 SVM</code> 은 위에서 <code class="language-plaintext highlighter-rouge">Margin</code> 과 <code class="language-plaintext highlighter-rouge">선형 SVM</code> 을 설명했으니 그리 어렵지 않습니다. 여기서 <code class="language-plaintext highlighter-rouge">Kernel Trick</code> 을 이용하는데요. 다시한번 <code class="language-plaintext highlighter-rouge">Kernel function</code> 형태를 보겠습니다.</p>

<p>[K(x, z) = \phi(x) \cdot \phi(z)]</p>

<p>그리고 이번에는 <code class="language-plaintext highlighter-rouge">2.3. Margin</code> 에서 내적이 나타나도록 수식을 바꾼 <code class="language-plaintext highlighter-rouge">Wolfe dual</code> 의 식을 한번 꺼내보겠습니다.</p>

<p>[\mathcal{L} = \sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i \alpha_j y_i y_j x_i \cdot x_j\]</p>

<p>이 수식은 L공간에서의 수식이고 이제 <code class="language-plaintext highlighter-rouge">Kernel trick</code> 형태에 맞게 H공간으로서 바꿔 작성하겠습니다.</p>

<p>[\mathcal{L} = \sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i \alpha_j y_i y_j \phi(x_i) \cdot \phi(x_j)\]</p>

<p>빌드업이 너무 잘되어 있어서 <code class="language-plaintext highlighter-rouge">비선형 SVM</code> 은 쉽게 넘어갈 수 있겠습니다.</p>

<h3 id="26-c-classification-svm"><span style="color:darkblue">2.6. c-classification SVM </span></h3>

<p><img src="/assets/img/MATLAB/10_8.png" alt="나루토분신술" /></p>

<p>지금까지 설명한 <code class="language-plaintext highlighter-rouge">SVM</code> 은 <code class="language-plaintext highlighter-rouge">Binomial classfication</code> 에 대한 이야기입니다. 두 집단에서 선형이냐, 비선형이냐로 싸운 것이죠. 근데 현실에서는 좀 더 여러 분류를 하다보니까 자주 쓰이는 <code class="language-plaintext highlighter-rouge">SVM</code> 은 <code class="language-plaintext highlighter-rouge">c-classification SVM</code> 입니다. 여러 종류의 기법이 있는데, 이에 대한 핵심은 <code class="language-plaintext highlighter-rouge">쌍</code> 을 통한 여백을 최대화 하는 것이므로 위에서 설명한 기존 <code class="language-plaintext highlighter-rouge">SVM</code> 을 복합적으로 활용한 사례입니다.</p>

<h3 id="27-svm-regression"><span style="color:darkblue">2.7. SVM regression </span></h3>

<p><img src="/assets/img/MATLAB/10_9.png" alt="기존회귀와소프트마진" /></p>

<p>보통은 <code class="language-plaintext highlighter-rouge">regression</code> 을 설명하고 <code class="language-plaintext highlighter-rouge">classification</code> 을 설명합니다. 왜냐하면 대체적으로 <code class="language-plaintext highlighter-rouge">regression</code> 이 좀 더 단순하기도 하고 <code class="language-plaintext highlighter-rouge">classification</code> 이 어렵고 다음 스텝의 느낌이 들기 때문입니다. 그러나 <code class="language-plaintext highlighter-rouge">SVM</code> 은 반대입니다. <code class="language-plaintext highlighter-rouge">분류먼저, 회귀는 분류의 확장개념</code>이죠.</p>

<p>여기서는 <code class="language-plaintext highlighter-rouge">둔감오류함수(insensitive error function)</code> 를 얘기합니다. 왜냐하면 일반 회귀식에서는 차원을 올릴 기반이 없고, 내적 형태가 나와야하기 때문입니다. <code class="language-plaintext highlighter-rouge">둔감오류함수</code> 는 에러를 <code class="language-plaintext highlighter-rouge">Margin</code> 값에 따라서 값을 정해줍니다.
\(\begin{array}{l}
E_\epsilon(y_i-f(x_i)) = \begin{cases}
0\qquad &amp; \text{if}\quad|y_i-f(x_i)|&lt;\epsilon \\
|y_i-f(x_i)|&lt;\epsilon &amp;\text{otherwise}
\end{cases}
\end{array}\)
<img src="/assets/img/MATLAB/10_10.png" alt="둔감오류함수" /></p>

<p>저희는 이제 <code class="language-plaintext highlighter-rouge">슬랙변수</code> 도 <code class="language-plaintext highlighter-rouge">Kernel trick</code> 도 아니까 <code class="language-plaintext highlighter-rouge">라그랑주 승수</code> 를 넘어서 바로 예측 수식에 들어가겠습니다.</p>

<p>[f(x)=\sum_{a_i\neq0,\hat{a_i}\neq0}(a_i-\hat{a_i})K(x,x_i)+b]</p>

<p>여기서 주요점은 <code class="language-plaintext highlighter-rouge">summation</code> 은 아래조건입니다. <code class="language-plaintext highlighter-rouge">결정경계</code> 에서 잘 분류된 샘플은 들어올 필요없는 것다는 점이며, 즉 이 수식에는 <code class="language-plaintext highlighter-rouge">Support vector</code> 만 들어간다는 점이죠. 이제 <code class="language-plaintext highlighter-rouge">MATLAB</code> 실습을 진행하겠습니다.</p>

<h2 id="3-matlab"><span style="color:darkblue">3. MATLAB </span></h2>

<p><code class="language-plaintext highlighter-rouge">MATLAB</code> 에서 모든 <code class="language-plaintext highlighter-rouge">Machine learning</code> 함수명은 <code class="language-plaintext highlighter-rouge">fit</code> 이 붙습니다. 그리고 <code class="language-plaintext highlighter-rouge">분류(classification)</code> 은 <code class="language-plaintext highlighter-rouge">c</code> 가, <code class="language-plaintext highlighter-rouge">회귀(regression)</code> 은 <code class="language-plaintext highlighter-rouge">r</code> 이 붙습니다. 마지막으로 <code class="language-plaintext highlighter-rouge">method</code> 명이 끝에 붙어서 <code class="language-plaintext highlighter-rouge">머신러닝 / 분류 / 디시젼트리</code> 라고 한다면, 함수명은 <code class="language-plaintext highlighter-rouge">fitctree</code> 가 됩니다. 또 <code class="language-plaintext highlighter-rouge">머신러닝 / 분류 / knn</code> 는 <code class="language-plaintext highlighter-rouge">fitcknn</code> 이구요. <code class="language-plaintext highlighter-rouge">머신러닝 / 회귀 / 디시젼트리</code> 는 <code class="language-plaintext highlighter-rouge">fitrtree</code> 입니다.</p>

<p><strong>그런데 SVM은 예외가 있습니다. OneClass SVM과 MultiClass SVM을 따로 분류합니다.</strong> 그래서 OCSVM은 <code class="language-plaintext highlighter-rouge">fitcsvm</code> 으로, MCSVM은 <code class="language-plaintext highlighter-rouge">fitcecoc</code> 로 사용합니다. 더 나아가 고차원 데이터 세트에서 <code class="language-plaintext highlighter-rouge">이진분류</code>는 <code class="language-plaintext highlighter-rouge">fitclinear</code> 를 얘기하고 있습니다. 회귀는 <code class="language-plaintext highlighter-rouge">fitrsvm</code> 으로 묶입니다. 그러나 마찬가지로 고차원은 <code class="language-plaintext highlighter-rouge">fitrlinear</code> 를 사용합니다. 그리고 아래는 <code class="language-plaintext highlighter-rouge">이진 다중 저차원 분류에서 고차원 분류로</code> 설명을 드리겠습니다.</p>

<h3 id="31-one-class-svm"><span style="color:darkblue">3.1. One class SVM </span></h3>

<p>첫번째로, <code class="language-plaintext highlighter-rouge">단일 클래스(One Class)</code> 혹은 <code class="language-plaintext highlighter-rouge">이진 분류(Binomial classification)</code> 에 적합한 함수인 <code class="language-plaintext highlighter-rouge">fitcsvm</code> 에 대한 예제입니다. 또한 높은 차원에 대해서는 이 함수가 적합하지 않습니다. <code class="language-plaintext highlighter-rouge">이진 분류</code> 라고 해도 차원이 많다면, 다른 함수를 써야합니다.</p>

<p>[분류]</p>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td> --><td class="rouge-code"><pre><span class="nb">load</span> <span class="n">fisheriris</span>
<span class="n">inds</span> <span class="o">=</span> <span class="o">~</span><span class="nb">strcmp</span><span class="p">(</span><span class="n">species</span><span class="p">,</span><span class="s1">'setosa'</span><span class="p">);</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">meas</span><span class="p">(</span><span class="n">inds</span><span class="p">,</span><span class="mi">3</span><span class="p">:</span><span class="mi">4</span><span class="p">);</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">species</span><span class="p">(</span><span class="n">inds</span><span class="p">);</span>

<span class="n">SVMModel</span> <span class="o">=</span> <span class="n">fitcsvm</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="n">classOrder</span> <span class="o">=</span> <span class="n">SVMModel</span><span class="o">.</span><span class="n">ClassNames</span>

<span class="n">sv</span> <span class="o">=</span> <span class="n">SVMModel</span><span class="o">.</span><span class="n">SupportVectors</span><span class="p">;</span>
<span class="nb">figure</span>
<span class="n">gscatter</span><span class="p">(</span><span class="n">X</span><span class="p">(:,</span><span class="mi">1</span><span class="p">),</span><span class="n">X</span><span class="p">(:,</span><span class="mi">2</span><span class="p">),</span><span class="n">y</span><span class="p">)</span>
<span class="nb">hold</span> <span class="n">on</span>
<span class="nb">plot</span><span class="p">(</span><span class="n">sv</span><span class="p">(:,</span><span class="mi">1</span><span class="p">),</span><span class="n">sv</span><span class="p">(:,</span><span class="mi">2</span><span class="p">),</span><span class="s1">'ko'</span><span class="p">,</span><span class="s1">'MarkerSize'</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="nb">legend</span><span class="p">(</span><span class="s1">'versicolor'</span><span class="p">,</span><span class="s1">'virginica'</span><span class="p">,</span><span class="s1">'Support Vector'</span><span class="p">)</span>
<span class="nb">hold</span> <span class="n">off</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>아무래도 <code class="language-plaintext highlighter-rouge">단일 클래스 분류</code> 라서, 분류할 두가지(<code class="language-plaintext highlighter-rouge">Versicolor</code>, <code class="language-plaintext highlighter-rouge">Verginica</code>)만 가져옵니다.</p>

<p><img src="/assets/img/MATLAB/10_11.png" alt="plot" /></p>

<p>[이상치, SupportVector 확인하기]</p>

<p><strong>2차 제작에 대한 출처 :</strong> <a href="https://kr.mathworks.com/help/stats/fitcsvm.html">mathworks help</a> , <a href="https://kr.mathworks.com/help/stats/examples/classification.html#d122e1480">fisheriris</a></p>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td> --><td class="rouge-code"><pre><span class="nb">load</span> <span class="n">fisheriris</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">meas</span><span class="p">(:,</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">);</span>
<span class="n">y</span> <span class="o">=</span> <span class="nb">ones</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="mi">1</span><span class="p">);</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">fisher의 iris</code> 데이터를 가져옵니다.<code class="language-plaintext highlighter-rouge"> 피셔의 붓꽃 데이터의 meas</code>는 150개 붓꽃 표본의 꽃받침 길이, 꽃받침 너비, 꽃잎 길이, 꽃잎 너비에 대한 측정값으로 구성되어 있습니다. 그러므로 <code class="language-plaintext highlighter-rouge">X</code> 에는 150개의 붓꽃 표본의 꽃받침 길이, 꽃받침 너비만 들어가 있습니다. 그리고 <code class="language-plaintext highlighter-rouge">Y</code> 는 150x1 짜리로 사이즈에 1로 구성된 열벡터가 들어가있습니다.</p>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td> --><td class="rouge-code"><pre><span class="nb">rng</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="n">SVMModel</span> <span class="o">=</span> <span class="n">fitcsvm</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="s1">'KernelScale'</span><span class="p">,</span><span class="s1">'auto'</span><span class="p">,</span><span class="s1">'Standardize'</span><span class="p">,</span><span class="nb">true</span><span class="p">,</span><span class="k">...</span>
    <span class="s1">'OutlierFraction'</span><span class="p">,</span><span class="mf">0.05</span><span class="p">);</span><span class="mi">0</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>수정된 데이터 세트를 사용하여 SVM 분류기를 훈련시킵니다. 관측값의 5%가 이상값이라고 가정합니다. 예측 변수를 표준화합니다.</p>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td> --><td class="rouge-code"><pre><span class="n">svInd</span> <span class="o">=</span> <span class="n">SVMModel</span><span class="o">.</span><span class="n">IsSupportVector</span><span class="p">;</span>
<span class="n">h</span> <span class="o">=</span> <span class="mf">0.02</span><span class="p">;</span> <span class="c1">% Mesh grid step size</span>
<span class="p">[</span><span class="n">X1</span><span class="p">,</span><span class="n">X2</span><span class="p">]</span> <span class="o">=</span> <span class="nb">meshgrid</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">X</span><span class="p">(:,</span><span class="mi">1</span><span class="p">)):</span><span class="n">h</span><span class="p">:</span><span class="nb">max</span><span class="p">(</span><span class="n">X</span><span class="p">(:,</span><span class="mi">1</span><span class="p">)),</span><span class="k">...</span>
    <span class="nb">min</span><span class="p">(</span><span class="n">X</span><span class="p">(:,</span><span class="mi">2</span><span class="p">)):</span><span class="n">h</span><span class="p">:</span><span class="nb">max</span><span class="p">(</span><span class="n">X</span><span class="p">(:,</span><span class="mi">2</span><span class="p">)));</span>
<span class="p">[</span><span class="o">~</span><span class="p">,</span><span class="n">score</span><span class="p">]</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">SVMModel</span><span class="p">,[</span><span class="n">X1</span><span class="p">(:),</span><span class="n">X2</span><span class="p">(:)]);</span>
<span class="n">scoreGrid</span> <span class="o">=</span> <span class="nb">reshape</span><span class="p">(</span><span class="n">score</span><span class="p">,</span><span class="nb">size</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="nb">size</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span><span class="mi">2</span><span class="p">));</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>훈련된 <code class="language-plaintext highlighter-rouge">SVMModel</code> 에서 시각화 준비를 합니다. 또한 <code class="language-plaintext highlighter-rouge">predict</code> 에서 분류된 레이블만이 아니라 유사도를 볼 수 있는 <code class="language-plaintext highlighter-rouge">score</code> 를 가져옵니다.</p>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td> --><td class="rouge-code"><pre><span class="nb">figure</span>
<span class="nb">plot</span><span class="p">(</span><span class="n">X</span><span class="p">(:,</span><span class="mi">1</span><span class="p">),</span><span class="n">X</span><span class="p">(:,</span><span class="mi">2</span><span class="p">),</span><span class="s1">'k.'</span><span class="p">)</span>
<span class="nb">hold</span> <span class="n">on</span>
<span class="nb">plot</span><span class="p">(</span><span class="n">X</span><span class="p">(</span><span class="n">svInd</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">X</span><span class="p">(</span><span class="n">svInd</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="s1">'ro'</span><span class="p">,</span><span class="s1">'MarkerSize'</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="nb">contour</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span><span class="n">X2</span><span class="p">,</span><span class="n">scoreGrid</span><span class="p">)</span>
<span class="nb">colorbar</span><span class="p">;</span>
<span class="nb">title</span><span class="p">(</span><span class="s1">'{\bf Iris Outlier Detection via One-Class SVM}'</span><span class="p">)</span>
<span class="nb">xlabel</span><span class="p">(</span><span class="s1">'Sepal Length (cm)'</span><span class="p">)</span>
<span class="nb">ylabel</span><span class="p">(</span><span class="s1">'Sepal Width (cm)'</span><span class="p">)</span>
<span class="nb">legend</span><span class="p">(</span><span class="s1">'Observation'</span><span class="p">,</span><span class="s1">'Support Vector'</span><span class="p">)</span>
<span class="nb">hold</span> <span class="n">off</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="/assets/img/MATLAB/10_12.png" alt="plot" /></p>

<p>이상값을 나머지 데이터와 분리하는 경계는 등고선 값이 <code class="language-plaintext highlighter-rouge">0</code>인 위치에서 나타납니다.</p>

<p>교차 검증된 데이터에서 음의 점수를 갖는 관측값의 비율이 5%에 가까운지 확인합니다.</p>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td> --><td class="rouge-code"><pre><span class="n">CVSVMModel</span> <span class="o">=</span> <span class="n">crossval</span><span class="p">(</span><span class="n">SVMModel</span><span class="p">);</span>
<span class="p">[</span><span class="o">~</span><span class="p">,</span><span class="n">scorePred</span><span class="p">]</span> <span class="o">=</span> <span class="n">kfoldPredict</span><span class="p">(</span><span class="n">CVSVMModel</span><span class="p">);</span>
<span class="n">outlierRate</span> <span class="o">=</span> <span class="nb">mean</span><span class="p">(</span><span class="n">scorePred</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>실제로 설정한 5%와 가까운지 <code class="language-plaintext highlighter-rouge">outlierRate</code> 가 알려줍니다.</p>

<h3 id="32-multi-class-svm"><span style="color:darkblue">3.2. Multi class SVM </span></h3>

<p>두번째는 <code class="language-plaintext highlighter-rouge">Multi class SVM</code>, <code class="language-plaintext highlighter-rouge">다중 분류, c-분류 SVM</code> 에 대한 예제입니다. <code class="language-plaintext highlighter-rouge">fitcecoc</code> 를 사용하고, 저~중 차원의 데이터에 적합합니다.</p>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td> --><td class="rouge-code"><pre><span class="nb">load</span> <span class="n">fisheriris</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">meas</span><span class="p">;</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">species</span><span class="p">;</span>
<span class="nb">rng</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span> <span class="c1">% For reproducibility</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>이번에는 피셔데이터를 몽땅씁니다. <code class="language-plaintext highlighter-rouge">다중분류</code> 라서 두 개만 고를 이유가 없거든요.</p>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td> --><td class="rouge-code"><pre><span class="n">t</span> <span class="o">=</span> <span class="n">templateSVM</span><span class="p">(</span><span class="s1">'Standardize'</span><span class="p">,</span><span class="nb">true</span><span class="p">)</span>
<span class="n">Mdl</span> <span class="o">=</span> <span class="n">fitcecoc</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="s1">'Learners'</span><span class="p">,</span><span class="n">t</span><span class="p">,</span><span class="k">...</span>
    <span class="s1">'ClassNames'</span><span class="p">,{</span><span class="s1">'setosa'</span><span class="p">,</span><span class="s1">'versicolor'</span><span class="p">,</span><span class="s1">'virginica'</span><span class="p">});</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">templateSVM</code> 를 이용하여 <code class="language-plaintext highlighter-rouge">모델 옵션</code> 을 만들어줍니디. 그리고 <code class="language-plaintext highlighter-rouge">fitcecoc</code> 를 통해 <code class="language-plaintext highlighter-rouge">model</code> 을 만듭니다.</p>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td> --><td class="rouge-code"><pre><span class="n">CVMdl</span> <span class="o">=</span> <span class="n">crossval</span><span class="p">(</span><span class="n">Mdl</span><span class="p">);</span>
<span class="n">genError</span> <span class="o">=</span> <span class="n">kfoldLoss</span><span class="p">(</span><span class="n">CVMdl</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">crossval</code> 은 교차검증이고 <code class="language-plaintext highlighter-rouge">kfoldLoss</code> 는 <code class="language-plaintext highlighter-rouge">k-fold</code> 검증입니다.둘 다 일반화된 분류 오차를 볼 수 있습니다.</p>

<h3 id="33-svm-for-high-dimension"><span style="color:darkblue">3.3. SVM for High dimension </span></h3>

<p>마지막으로 <code class="language-plaintext highlighter-rouge">CNN</code> 과 결합하여 사용할때 주로 고차원이 만들어지는데, 이 외에도 많은 변수들을 사용하여 예측할 때 쓰는 <code class="language-plaintext highlighter-rouge">fitclinear / fitrlinear</code> 입니다. 이 챕터는 번역된 자료가 없어서 제가 번역을 하여 2차 창작을 했습니다.</p>

<p><a href="https://kr.mathworks.com/help/stats/fitclinear.html">Mathworks help</a></p>

<h4 id="331-description"><span style="color:darkblue">3.3.1. description </span></h4>

<p><code class="language-plaintext highlighter-rouge">fitclinear</code> 는 선형 분류 모델로 <code class="language-plaintext highlighter-rouge">고차원 이진분류</code>, <code class="language-plaintext highlighter-rouge">많은 예측 데이터</code> , 정규화된 <code class="language-plaintext highlighter-rouge">SVM</code> 에서의 선형분리가 가능한 모델, <code class="language-plaintext highlighter-rouge">로지스틱 회귀</code> 모델에 사용합니다. <code class="language-plaintext highlighter-rouge">fitclinear</code> 는 <code class="language-plaintext highlighter-rouge">스토캐스틱 경사하강법</code> 같은 기술로서 적은 연산시간으로 목적함수 최소화를 진행합니다. 뭐 어쨌든 많은 변수가 있으면 고차원이고, 그 고차원에서는 아무래도 연산량이 늘어나다보니 <code class="language-plaintext highlighter-rouge">fitclinear</code> 는 고차원에서 생기는 느려지는 연산에 초점을 맞춘 함수입니다.</p>

<h4 id="332-train-linear-classification-model-"><span style="color:darkblue">3.3.2. Train Linear Classification Model </span></h4>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td> --><td class="rouge-code"><pre><span class="nb">load</span> <span class="n">nlpdata</span>
<span class="n">Ystats</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">==</span> <span class="s1">'stats'</span><span class="p">;</span>
<span class="n">Lambda</span> <span class="o">=</span> <span class="nb">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mi">11</span><span class="p">);</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">NLP dataset</code> 을 이용하구요 . X는 예측변수들에 대한 희소행렬입니다. Y는 종속변수이면서 레이블입니다. 그리고 <code class="language-plaintext highlighter-rouge">Lambda</code> 의 <code class="language-plaintext highlighter-rouge">logspace</code> 는 11개의 10^-6 에서 10^-0.5 사이의 로그간격 값을 만듭니다.</p>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td> --><td class="rouge-code"><pre><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">'</span><span class="p">;</span> 
<span class="nb">rng</span><span class="p">(</span><span class="mi">10</span><span class="p">);</span> <span class="c1">% For reproducibility</span>
<span class="n">CVMdl</span> <span class="o">=</span> <span class="n">fitclinear</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Ystats</span><span class="p">,</span><span class="s1">'ObservationsIn'</span><span class="p">,</span><span class="s1">'columns'</span><span class="p">,</span><span class="s1">'KFold'</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="k">...</span>
    <span class="s1">'Learner'</span><span class="p">,</span><span class="s1">'svm'</span><span class="p">,</span><span class="s1">'Solver'</span><span class="p">,</span><span class="s1">'dual'</span><span class="p">,</span><span class="s1">'Regularization'</span><span class="p">,</span><span class="s1">'ridge'</span><span class="p">,</span><span class="k">...</span>
    <span class="s1">'Lambda'</span><span class="p">,</span><span class="n">Lambda</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">CVMdl</code> 이라는 모델을 만들었구요. <code class="language-plaintext highlighter-rouge">Learner</code> 는 <code class="language-plaintext highlighter-rouge">svm</code> , <code class="language-plaintext highlighter-rouge">Solver</code> 는 <code class="language-plaintext highlighter-rouge">dual</code>, <code class="language-plaintext highlighter-rouge">Regulation</code> 은 <code class="language-plaintext highlighter-rouge">ridge</code> 로 설정합니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
</pre></td> --><td class="rouge-code"><pre>numCLModels = numel(CVMdl.Trained)
</pre></td></tr></tbody></table></code></pre></div></div>

<p>5겹 교차검증(<code class="language-plaintext highlighter-rouge">KFold</code>, 5) 로 설정해서 훈련된 <code class="language-plaintext highlighter-rouge">CVMdl</code> 부분 모델이 5개가 나왔나 확인해봅니다.</p>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
</pre></td> --><td class="rouge-code"><pre><span class="n">Mdl1</span> <span class="o">=</span> <span class="n">CVMdl</span><span class="o">.</span><span class="n">Trained</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>그 중 하나만 잡아와 특징을 확인하구요.</p>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
</pre></td> --><td class="rouge-code"><pre><span class="n">ce</span> <span class="o">=</span> <span class="n">kfoldLoss</span><span class="p">(</span><span class="n">CVMdl</span><span class="p">);</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">cross validation error</code> 를 확인합니다.</p>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td> --><td class="rouge-code"><pre><span class="n">Mdl</span> <span class="o">=</span> <span class="n">fitclinear</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Ystats</span><span class="p">,</span><span class="s1">'ObservationsIn'</span><span class="p">,</span><span class="s1">'columns'</span><span class="p">,</span><span class="k">...</span>
    <span class="s1">'Learner'</span><span class="p">,</span><span class="s1">'svm'</span><span class="p">,</span><span class="s1">'Solver'</span><span class="p">,</span><span class="s1">'dual'</span><span class="p">,</span><span class="s1">'Regularization'</span><span class="p">,</span><span class="s1">'ridge'</span><span class="p">,</span><span class="k">...</span>
    <span class="s1">'Lambda'</span><span class="p">,</span><span class="n">Lambda</span><span class="p">);</span>
<span class="n">numNZCoeff</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">Mdl</span><span class="o">.</span><span class="n">Beta</span><span class="o">~=</span><span class="mi">0</span><span class="p">);</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>아까 봤던 것은 <code class="language-plaintext highlighter-rouge">K-fold</code> 옵션을 넣은 것이구요. 이번에는 빼보겠습니다.</p>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td> --><td class="rouge-code"><pre><span class="nb">figure</span><span class="p">;</span>
<span class="p">[</span><span class="n">h</span><span class="p">,</span><span class="n">hL1</span><span class="p">,</span><span class="n">hL2</span><span class="p">]</span> <span class="o">=</span> <span class="nb">plotyy</span><span class="p">(</span><span class="nb">log10</span><span class="p">(</span><span class="n">Lambda</span><span class="p">),</span><span class="nb">log10</span><span class="p">(</span><span class="n">ce</span><span class="p">),</span><span class="k">...</span>
    <span class="nb">log10</span><span class="p">(</span><span class="n">Lambda</span><span class="p">),</span><span class="nb">log10</span><span class="p">(</span><span class="n">numNZCoeff</span><span class="p">));</span> 
<span class="n">hL1</span><span class="o">.</span><span class="n">Marker</span> <span class="o">=</span> <span class="s1">'o'</span><span class="p">;</span>
<span class="n">hL2</span><span class="o">.</span><span class="n">Marker</span> <span class="o">=</span> <span class="s1">'o'</span><span class="p">;</span>
<span class="nb">ylabel</span><span class="p">(</span><span class="n">h</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span><span class="s1">'log_{10} classification error'</span><span class="p">)</span>
<span class="nb">ylabel</span><span class="p">(</span><span class="n">h</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span><span class="s1">'log_{10} nonzero-coefficient frequency'</span><span class="p">)</span>
<span class="nb">xlabel</span><span class="p">(</span><span class="s1">'log_{10} Lambda'</span><span class="p">)</span>
<span class="nb">title</span><span class="p">(</span><span class="s1">'Test-Sample Statistics'</span><span class="p">)</span>
<span class="nb">hold</span> <span class="n">off</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="/assets/img/MATLAB/10_13.png" alt="plot" /></p>
:ET